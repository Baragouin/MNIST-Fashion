{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# NEURAL NETWORK \n",
    "## MNIST FASHION DATASET\n",
    "\n",
    "![Visualization](assets/visualization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Goal\n",
    "\n",
    "This project was realized during our competency building for the [MindGame](https://github.com/PoCInnovation/MindGame) project with [PoCInnovation](https://github.com/PoCInnovation)\n",
    "\n",
    "[![PocInnovation](assets/poc.jpeg)](https://github.com/PoCInnovation)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset\n",
    "Zalando Research Project: [Github](https://github.com/zalandoresearch/fashion-mnist)\n",
    "<br>\n",
    "\n",
    "> ## Why we made Fashion-MNIST\n",
    ">\n",
    "> The original [MNIST dataset](http://yann.lecun.com/exdb/mnist/) contains a lot of handwritten digits. Members of the AI/ML/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset researchers try. *\"If it doesn't work on MNIST, it **won't work** at all\"*, they said. *\"Well, if it does work on MNIST, it may still fail on others.\"*\n",
    ">\n",
    "> ### To Serious Machine Learning Researchers\n",
    ">\n",
    "> Seriously, we are talking about replacing MNIST. Here are some good reasons:\n",
    ">\n",
    "> - **MNIST is too easy.** Convolutional nets can achieve 99.7% on MNIST. Classic machine learning algorithms can also achieve 97% easily. Check out [our side-by-side benchmark for Fashion-MNIST vs. MNIST](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/), and read \"[Most pairs of MNIST digits can be distinguished pretty well by just one pixel](https://gist.github.com/dgrtwo/aaef94ecc6a60cd50322c0054cc04478).\"\n",
    "> - **MNIST is overused.** In [this April 2017 Twitter thread](https://twitter.com/goodfellow_ian/status/852591106655043584), Google Brain research scientist and deep learning expert Ian Goodfellow calls for people to move away from MNIST.\n",
    "> - **MNIST can not represent modern CV tasks**, as noted in [this April 2017 Twitter thread](https://twitter.com/fchollet/status/852594987527045120), deep learning expert/Keras author François Chollet.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Contributors\n",
    "\n",
    "| Developer | Developer | Manager |\n",
    "|:----------:|:----------:|:----------:|\n",
    "| [<img src=\"https://github.com/Baragouin.png?size=85\" width=85><br><sub>Timothé Medico</sub>](https://github.com/Baragouin) |  [<img src=\"https://github.com/agherasie.png?size=85\" width=85><br><sub>Alexandru Gherasie</sub>](https://github.com/agherasie) | [<img src=\"https://github.com/Mikatech.png?size=85\" width=85><br><sub>Mikaël Vallenet</sub>](https://github.com/Mikatech) |"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prerequisites\n",
    "\n",
    "Installation of missing dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "% pip install torch torchvision numpy matplotlib tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importation of the necessary modules"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creation of the *DeepLearning* model\n",
    "\n",
    "We create a PyTorch Module to define our model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Convolution layer\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.max_pool = nn.MaxPool2d(2, 2)\n",
    "        # --------------------------------\n",
    "\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.linear1 = nn.Linear(256, 120)\n",
    "        self.linear2 = nn.Linear(120, 10)\n",
    "        # --------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Convolution layer\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.max_pool(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.max_pool(x)\n",
    "        # -----------------------\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Flatten filters\n",
    "        x = x.view(-1, 256)\n",
    "\n",
    "        # Fully connected layer\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear2(x))\n",
    "        # -------------------------\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create the instance of our model and load CUDA if available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "network = NeuralNetwork()\n",
    "\n",
    "# Load CUDA if available\n",
    "if torch.cuda.is_available():\n",
    "    network.cuda()\n",
    "print(\"Use {}\".format(['GPU with CUDA' if torch.cuda.is_available() else 'CPU']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataloading\n",
    "\n",
    "Defining global macros and loading data into a `DataLoader` by *batch*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constant values\n",
    "EPOCH = 32\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Convert and normalize input images\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load data\n",
    "train_set = torchvision.datasets.FashionMNIST(root='data', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.FashionMNIST(root='data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Select a batch of data\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define label names\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load a loss calculator and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(network.parameters(), lr=LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def img_show(img):\n",
    "    img = img / 2 + .5\n",
    "    np_img = img.numpy()\n",
    "    plt.imshow(np.transpose(np_img, (1, 2, 0)))\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training and monitoring\n",
    "\n",
    "Training is done with CUDA if available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "iteration = 0\n",
    "start_time = time.time()\n",
    "\n",
    "train_accuracies = np.zeros(EPOCH)\n",
    "test_accuracies = np.zeros(EPOCH)\n",
    "\n",
    "for iteration in range(EPOCH):\n",
    "    average_loss = 0.0\n",
    "\n",
    "    print(\"――――――――――――――――――――――――――――――――――――――――――――――――――\")\n",
    "\n",
    "    # Training\n",
    "    total = 0\n",
    "    success = 0\n",
    "    for inputs, labels in tqdm(train_loader):\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        optimizer.zero_grad()\n",
    "        output = network.forward(inputs)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #average_loss += loss.data[0]\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        success += (predicted == labels.data).sum()\n",
    "    train_accuracies[iteration] = 100.0 * success / total\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    # Testing\n",
    "    total = 0\n",
    "    success = 0\n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "        output = network.forward(inputs)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += labels.size(0)\n",
    "        success += (predicted == labels.data).sum()\n",
    "    test_accuracies[iteration] = 100.0 * success / total\n",
    "    # -------------------------------------------------------------------\n",
    "\n",
    "    print(u\"Epoch {}, average loss {}, train accuracy {}, test accuracy {}\".format(\n",
    "        iteration,\n",
    "        average_loss / len(train_loader),\n",
    "        train_accuracies[iteration],\n",
    "        test_accuracies[iteration]\n",
    "    ))\n",
    "\n",
    "seconds = time.time() - start_time\n",
    "duration = datetime.time(second=seconds)\n",
    "display = duration.strftime(\"%Mm %Ss\")\n",
    "\n",
    "print(\"――――――――――――――――――――――――――――――――――――――――――――――――――\")\n",
    "print(\"Training time in seconds: {}\".format(display))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Display accuracies across epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, EPOCH + 1), train_accuracies)\n",
    "plt.plot(np.arange(1, EPOCH + 1), test_accuracies)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}